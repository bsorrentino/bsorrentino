---
layout: post
title:  "PlantUML+GPTÂ - Visual Studio Extension with AI Multi-Modality"
date:   2024-05-06 08:00:00 +0200
categories: app
---
![cover](../../../../assets/plantuml-app/extension-cover.png)
<br>

Continuing my commitment to using the LLM ([OpenAI/GPT][OpenAI]) to help me draft diagrams in [PlantUML], which started with the release of [PlantUMLApp], led me to release a new update of the **[PlantUML+GPT Visual Studio Code Extension][vsce]**.Â Â 

## MultimodalityÂ 

Version `0.4.x` now includes the new multi-modality feature. This update allows you to use the [Vision] model to transform diagram images from the internet or your workspace into [PlantUML] scripts. Check out the animated GIF below.

![](../../../../assets/plantuml-app/create-diagram-from-image.gif)Â 

## The Multi Agents CollaborationÂ 

In this extension I've used Multi-Agents-Collaboration using the [LangGraph]Â framework from the awesome [Langchain] project, applying the process shown in the diagram below:Â 

![](../../../../assets/plantuml-app/drawing-to-diagram.png)Â 

As you can see, I've used three Agents with different capabilities:

### Agent Vision Â 

This agent can process images, it is skilled in describing a diagram producing a structured output containing also the diagram typology useful to involve the right Agent for further processing.Â 

> This increase the flexibility of system because the image is translated in structured data that can be processed by agents with different skills and goalsÂ 

### Agent Translator(1)Â 

This agent is skilled in PlantUML sequence diagram. It gets the diagram data and translate them in the PlantUML scriptÂ 

### Agent Translator(2)Â 

This agent is skilled in PlantUML generic process diagram. It gets the diagram data and translate them in PlantUML scriptÂ 

## Conclusion Â Â 

Multi-Modality enhances AI's ability to understand various types of information encountered in daily life. Utilizing this capability can significantly boost the efficiency of your AI system. I am currently exploring this and plan to introduce an auto-correction process for errors,  providing feedback to agents.

So, stay tuned! In the meanwhile, enjoy coding! ðŸ‘‹ Â 

[vsce]: https://marketplace.visualstudio.com/items?itemName=bsorrentino.plantuml-gpt
[PlanrUML]: https://plantuml.com/
[PlantUMLApp]: https://bsorrentino.github.io/bsorrentino/app/2024/04/08/PlantUMLApp-3.html
[LangChain]: https://js.langchain.com/docs/get_started/introduction
[LangGraph]: https://js.langchain.com/docs/langgraph
[OpenAI]: https://openai.com/api
[Vision]: https://platform.openai.com/docs/guides/vision